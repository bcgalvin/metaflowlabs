[{"data":1,"prerenderedAt":1189},["ShallowReactive",2],{"learn-content-/learn/deep-dives/persistence/datastore-internals":3,"learn-/learn/deep-dives/persistence/datastore-internals-articles":1177,"learn-surround-/learn/deep-dives/persistence/datastore-internals":1178},{"id":4,"title":5,"body":6,"description":1168,"extension":1169,"meta":1170,"navigation":141,"navigationTitle":1171,"order":138,"path":1172,"seo":1173,"stem":1174,"surround":141,"titleTemplate":1175,"__hash__":1176},"learn/1.learn/1.deep-dives/1.persistence/2.datastore-internals.md","Metaflow Datastore Internals: Caching and Resume",{"type":7,"value":8,"toc":1156},"minimal",[9,14,18,23,35,38,67,73,76,83,90,94,780,783,788,791,870,873,876,882,885,890,893,906,910,969,977,980,986,989,992,995,999,1002,1005,1016,1019,1023,1026,1054,1058,1078,1083,1086,1092,1095,1098,1101,1105,1108,1131,1135,1138,1149,1152],[10,11,13],"h1",{"id":12},"understanding-metaflows-datastore-internals-of-caching-and-resume","Understanding Metaflow's Datastore: Internals of Caching and Resume",[15,16,17],"p",{},"Metaflow's interaction with its configured datastore isn't just a background detail; it's the engine driving some of its most powerful features—like robust data versioning, highly efficient caching, and the life-saving ability to resume flows. If you've ever wondered how Metaflow seems to magically pick up where it left off, or how it avoids recomputing steps unnecessarily, a look into its datastore mechanics will provide the answers. Let's delve into how Metaflow works with data behind the scenes.",[19,20,22],"h2",{"id":21},"the-power-of-content-addressed-storage","The Power of Content-Addressed Storage",[15,24,25,26,30,31,34],{},"At the heart of Metaflow's data management is ",[27,28,29],"strong",{},"Content-Addressed Storage (CAS)",", this is the mechanism that fundamentally shapes how Metaflow handles every piece of data—or ",[27,32,33],{},"artifact","—you create. In the CAS model, each artifact isn't identified by a filename or a path you choose, but by a unique cryptographic hash computed directly from its serialized content. Think of this hash as the artifact's fingerprint; it becomes its \"address\" or key within Metaflow's datastore.",[15,36,37],{},"Why is this approach so effective? It offers several key benefits that you'll appreciate in your daily work:",[39,40,41,52],"ul",{},[42,43,44,47,48,51],"li",{},[27,45,46],{},"Automatic Deduplication",": Imagine your flow processes a large, static dataset in multiple steps, or perhaps you re-run a flow that uses the same initial data. If different steps or even related flow runs produce the exact same data (meaning they have the identical content hash), Metaflow is smart enough to store this data ",[27,49,50],{},"only once",". This can lead to significant storage savings, especially when you're dealing with large datasets or complex models that might be shared or regenerated.",[42,53,54,57,58,62,63,66],{},[27,55,56],{},"Built-in Immutability and Versioning",": Since an artifact's identity ",[59,60,61],"em",{},"is"," its content hash, any change to an object—no matter how tiny—will result in a brand-new hash. This means a new, distinct artifact is created in the datastore. The original remains untouched. This inherent ",[27,64,65],{},"immutability"," is crucial; it's the bedrock of Metaflow's robust versioning capabilities, ensuring that past results are stable and always reproducible.",[68,69,70],"note",{},[15,71,72],{},"The concept of immutability in content-addressing is a powerful guarantee. Once an artifact is written to the datastore, it's set in stone. Any apparent \"change\" actually results in a new, versioned artifact, leaving the original intact. This is what allows you to confidently look back at past runs, knowing the data you see is exactly what was produced then. This stability is fundamental to tracking lineage and managing different data versions throughout your project's lifecycle.",[15,74,75],{},"To visualize this, consider how different data objects map to unique storage locations based on their content:",[15,77,78],{},[79,80],"img",{"alt":81,"src":82},"Diagram illustrating content-addressed storage: different data objects with unique content hashes are stored, while identical objects share the same storage.","/content/learn/datastore-internals-01.png",[15,84,85,86,89],{},"It's not just your data artifacts that benefit from this. Metaflow also applies content-addressing to snapshot the ",[27,87,88],{},"code package"," associated with each run. This entire package (your flow script and any local modules) is bundled, hashed, and stored in the datastore. This ensures that you can always retrieve the exact version of the code that produced a specific set of artifacts, further bolstering reproducibility.",[19,91,93],{"id":92},"the-artifact-lifecycle-from-creation-to-datastore-and-back","The Artifact Lifecycle: From Creation to Datastore and Back",[95,96,102],"pre",{"className":97,"code":98,"language":99,"meta":100,"style":101},"language-python shiki shiki-themes material-theme-lighter material-theme-palenight","from metaflow import FlowSpec, step, current\n\nclass MyIllustrativeFlow(FlowSpec):\n  @step\n  def start(self):\n    print(f\"Starting flow {current.run_id}...\")\n    self.next(self.process_data)\n\n  @step\n  def process_data(self):\n    # 1. Creation & Assignment: An object is created in memory.\n    raw_data = [1, 2, 3, 4, 5]\n    processed_data_object = [x * 2 for x in raw_data]\n\n    # This assignment tells Metaflow to manage 'my_processed_data'.\n    self.my_processed_data = processed_data_object\n    # self.my_processed_data now holds [2, 4, 6, 8, 10] in memory.\n\n    print(f\"Step '{current.step_name}': Data processed: {self.my_processed_data}\")\n    self.next(self.use_data)\n\n  @step\n  def use_data(self):\n\n    # 6. Access: Metaflow retrieves 'my_processed_data' for this step.\n    #    - Metadata service is queried for the artifact's location (hash).\n    #    - Byte stream is read from the datastore.\n    #    - Deserialized back into a Python object.\n    retrieved_data = self.my_processed_data\n    # retrieved_data is now [2, 4, 6, 8, 10], loaded from the datastore.\n\n    print(f\"Step '{current.step_name}': Using data: {retrieved_data}\")\n    doubled_again = [x * 2 for x in retrieved_data]\n    self.final_result = doubled_again\n    self.next(self.end)\n\n  @step\n  def end(self):\n\n    # 'final_result' would also be persisted after this step completes.\n    print(f\"Step '{current.step_name}': Final result: {self.final_result}\")\n    print(f\"Flow {current.run_id} finished.\")\n\n# Upon step 'process_data' completion:\n# 2. Serialization: `self.my_processed_data` (the list [2, 4, 6, 8, 10]) is serialized (e.g., pickled).\n# 3. Hashing: A content hash (e.g., SHA1) is computed from the serialized bytes.\n# 4. Persistence (Write to Datastore): The serialized bytes are written to the datastore,\n#    often using the hash as part of its storage key. This is a datastore write operation.\n# 5. Metadata Update: Metaflow's metadata service records info about this artifact:\n#    flow, run ID, step name, task ID, and its content hash.\n","python","twoslash","",[103,104,105,136,143,163,173,190,228,250,255,262,276,283,321,353,358,364,380,386,391,432,452,457,464,478,483,489,495,501,507,523,529,534,569,596,611,631,636,643,657,662,668,706,733,738,744,750,756,762,768,774],"code",{"__ignoreMap":101},[106,107,110,114,118,121,124,128,131,133],"span",{"class":108,"line":109},"line",1,[106,111,113],{"class":112},"sNZSE","from",[106,115,117],{"class":116},"sv0dM"," metaflow ",[106,119,120],{"class":112},"import",[106,122,123],{"class":116}," FlowSpec",[106,125,127],{"class":126},"s4Nzc",",",[106,129,130],{"class":116}," step",[106,132,127],{"class":126},[106,134,135],{"class":116}," current\n",[106,137,139],{"class":108,"line":138},2,[106,140,142],{"emptyLinePlaceholder":141},true,"\n",[106,144,146,150,154,157,160],{"class":108,"line":145},3,[106,147,149],{"class":148},"sBv9x","class",[106,151,153],{"class":152},"s2dLm"," MyIllustrativeFlow",[106,155,156],{"class":126},"(",[106,158,159],{"class":152},"FlowSpec",[106,161,162],{"class":126},"):\n",[106,164,166,169],{"class":108,"line":165},4,[106,167,168],{"class":126},"  @",[106,170,172],{"class":171},"sp2xM","step\n",[106,174,176,179,182,184,188],{"class":108,"line":175},5,[106,177,178],{"class":148},"  def",[106,180,181],{"class":171}," start",[106,183,156],{"class":126},[106,185,187],{"class":186},"swAhH","self",[106,189,162],{"class":126},[106,191,193,196,198,201,205,209,212,215,219,222,225],{"class":108,"line":192},6,[106,194,195],{"class":171},"    print",[106,197,156],{"class":126},[106,199,200],{"class":148},"f",[106,202,204],{"class":203},"s9-rc","\"Starting flow ",[106,206,208],{"class":207},"sRqct","{",[106,210,211],{"class":171},"current",[106,213,214],{"class":126},".",[106,216,218],{"class":217},"sbA4_","run_id",[106,220,221],{"class":207},"}",[106,223,224],{"class":203},"...\"",[106,226,227],{"class":126},")\n",[106,229,231,234,236,239,241,243,245,248],{"class":108,"line":230},7,[106,232,233],{"class":116},"    self",[106,235,214],{"class":126},[106,237,238],{"class":171},"next",[106,240,156],{"class":126},[106,242,187],{"class":116},[106,244,214],{"class":126},[106,246,247],{"class":217},"process_data",[106,249,227],{"class":126},[106,251,253],{"class":108,"line":252},8,[106,254,142],{"emptyLinePlaceholder":141},[106,256,258,260],{"class":108,"line":257},9,[106,259,168],{"class":126},[106,261,172],{"class":171},[106,263,265,267,270,272,274],{"class":108,"line":264},10,[106,266,178],{"class":148},[106,268,269],{"class":171}," process_data",[106,271,156],{"class":126},[106,273,187],{"class":186},[106,275,162],{"class":126},[106,277,279],{"class":108,"line":278},11,[106,280,282],{"class":281},"s742E","    # 1. Creation & Assignment: An object is created in memory.\n",[106,284,286,289,292,295,298,300,303,305,308,310,313,315,318],{"class":108,"line":285},12,[106,287,288],{"class":116},"    raw_data ",[106,290,291],{"class":126},"=",[106,293,294],{"class":126}," [",[106,296,297],{"class":207},"1",[106,299,127],{"class":126},[106,301,302],{"class":207}," 2",[106,304,127],{"class":126},[106,306,307],{"class":207}," 3",[106,309,127],{"class":126},[106,311,312],{"class":207}," 4",[106,314,127],{"class":126},[106,316,317],{"class":207}," 5",[106,319,320],{"class":126},"]\n",[106,322,324,327,329,331,334,337,339,342,345,348,351],{"class":108,"line":323},13,[106,325,326],{"class":116},"    processed_data_object ",[106,328,291],{"class":126},[106,330,294],{"class":126},[106,332,333],{"class":116},"x ",[106,335,336],{"class":126},"*",[106,338,302],{"class":207},[106,340,341],{"class":112}," for",[106,343,344],{"class":116}," x ",[106,346,347],{"class":112},"in",[106,349,350],{"class":116}," raw_data",[106,352,320],{"class":126},[106,354,356],{"class":108,"line":355},14,[106,357,142],{"emptyLinePlaceholder":141},[106,359,361],{"class":108,"line":360},15,[106,362,363],{"class":281},"    # This assignment tells Metaflow to manage 'my_processed_data'.\n",[106,365,367,369,371,374,377],{"class":108,"line":366},16,[106,368,233],{"class":116},[106,370,214],{"class":126},[106,372,373],{"class":217},"my_processed_data",[106,375,376],{"class":126}," =",[106,378,379],{"class":116}," processed_data_object\n",[106,381,383],{"class":108,"line":382},17,[106,384,385],{"class":281},"    # self.my_processed_data now holds [2, 4, 6, 8, 10] in memory.\n",[106,387,389],{"class":108,"line":388},18,[106,390,142],{"emptyLinePlaceholder":141},[106,392,394,396,398,400,403,405,407,409,412,414,417,419,421,423,425,427,430],{"class":108,"line":393},19,[106,395,195],{"class":171},[106,397,156],{"class":126},[106,399,200],{"class":148},[106,401,402],{"class":203},"\"Step '",[106,404,208],{"class":207},[106,406,211],{"class":171},[106,408,214],{"class":126},[106,410,411],{"class":217},"step_name",[106,413,221],{"class":207},[106,415,416],{"class":203},"': Data processed: ",[106,418,208],{"class":207},[106,420,187],{"class":116},[106,422,214],{"class":126},[106,424,373],{"class":217},[106,426,221],{"class":207},[106,428,429],{"class":203},"\"",[106,431,227],{"class":126},[106,433,435,437,439,441,443,445,447,450],{"class":108,"line":434},20,[106,436,233],{"class":116},[106,438,214],{"class":126},[106,440,238],{"class":171},[106,442,156],{"class":126},[106,444,187],{"class":116},[106,446,214],{"class":126},[106,448,449],{"class":217},"use_data",[106,451,227],{"class":126},[106,453,455],{"class":108,"line":454},21,[106,456,142],{"emptyLinePlaceholder":141},[106,458,460,462],{"class":108,"line":459},22,[106,461,168],{"class":126},[106,463,172],{"class":171},[106,465,467,469,472,474,476],{"class":108,"line":466},23,[106,468,178],{"class":148},[106,470,471],{"class":171}," use_data",[106,473,156],{"class":126},[106,475,187],{"class":186},[106,477,162],{"class":126},[106,479,481],{"class":108,"line":480},24,[106,482,142],{"emptyLinePlaceholder":141},[106,484,486],{"class":108,"line":485},25,[106,487,488],{"class":281},"    # 6. Access: Metaflow retrieves 'my_processed_data' for this step.\n",[106,490,492],{"class":108,"line":491},26,[106,493,494],{"class":281},"    #    - Metadata service is queried for the artifact's location (hash).\n",[106,496,498],{"class":108,"line":497},27,[106,499,500],{"class":281},"    #    - Byte stream is read from the datastore.\n",[106,502,504],{"class":108,"line":503},28,[106,505,506],{"class":281},"    #    - Deserialized back into a Python object.\n",[106,508,510,513,515,518,520],{"class":108,"line":509},29,[106,511,512],{"class":116},"    retrieved_data ",[106,514,291],{"class":126},[106,516,517],{"class":116}," self",[106,519,214],{"class":126},[106,521,522],{"class":217},"my_processed_data\n",[106,524,526],{"class":108,"line":525},30,[106,527,528],{"class":281},"    # retrieved_data is now [2, 4, 6, 8, 10], loaded from the datastore.\n",[106,530,532],{"class":108,"line":531},31,[106,533,142],{"emptyLinePlaceholder":141},[106,535,537,539,541,543,545,547,549,551,553,555,558,560,563,565,567],{"class":108,"line":536},32,[106,538,195],{"class":171},[106,540,156],{"class":126},[106,542,200],{"class":148},[106,544,402],{"class":203},[106,546,208],{"class":207},[106,548,211],{"class":171},[106,550,214],{"class":126},[106,552,411],{"class":217},[106,554,221],{"class":207},[106,556,557],{"class":203},"': Using data: ",[106,559,208],{"class":207},[106,561,562],{"class":171},"retrieved_data",[106,564,221],{"class":207},[106,566,429],{"class":203},[106,568,227],{"class":126},[106,570,572,575,577,579,581,583,585,587,589,591,594],{"class":108,"line":571},33,[106,573,574],{"class":116},"    doubled_again ",[106,576,291],{"class":126},[106,578,294],{"class":126},[106,580,333],{"class":116},[106,582,336],{"class":126},[106,584,302],{"class":207},[106,586,341],{"class":112},[106,588,344],{"class":116},[106,590,347],{"class":112},[106,592,593],{"class":116}," retrieved_data",[106,595,320],{"class":126},[106,597,599,601,603,606,608],{"class":108,"line":598},34,[106,600,233],{"class":116},[106,602,214],{"class":126},[106,604,605],{"class":217},"final_result",[106,607,376],{"class":126},[106,609,610],{"class":116}," doubled_again\n",[106,612,614,616,618,620,622,624,626,629],{"class":108,"line":613},35,[106,615,233],{"class":116},[106,617,214],{"class":126},[106,619,238],{"class":171},[106,621,156],{"class":126},[106,623,187],{"class":116},[106,625,214],{"class":126},[106,627,628],{"class":217},"end",[106,630,227],{"class":126},[106,632,634],{"class":108,"line":633},36,[106,635,142],{"emptyLinePlaceholder":141},[106,637,639,641],{"class":108,"line":638},37,[106,640,168],{"class":126},[106,642,172],{"class":171},[106,644,646,648,651,653,655],{"class":108,"line":645},38,[106,647,178],{"class":148},[106,649,650],{"class":171}," end",[106,652,156],{"class":126},[106,654,187],{"class":186},[106,656,162],{"class":126},[106,658,660],{"class":108,"line":659},39,[106,661,142],{"emptyLinePlaceholder":141},[106,663,665],{"class":108,"line":664},40,[106,666,667],{"class":281},"    # 'final_result' would also be persisted after this step completes.\n",[106,669,671,673,675,677,679,681,683,685,687,689,692,694,696,698,700,702,704],{"class":108,"line":670},41,[106,672,195],{"class":171},[106,674,156],{"class":126},[106,676,200],{"class":148},[106,678,402],{"class":203},[106,680,208],{"class":207},[106,682,211],{"class":171},[106,684,214],{"class":126},[106,686,411],{"class":217},[106,688,221],{"class":207},[106,690,691],{"class":203},"': Final result: ",[106,693,208],{"class":207},[106,695,187],{"class":116},[106,697,214],{"class":126},[106,699,605],{"class":217},[106,701,221],{"class":207},[106,703,429],{"class":203},[106,705,227],{"class":126},[106,707,709,711,713,715,718,720,722,724,726,728,731],{"class":108,"line":708},42,[106,710,195],{"class":171},[106,712,156],{"class":126},[106,714,200],{"class":148},[106,716,717],{"class":203},"\"Flow ",[106,719,208],{"class":207},[106,721,211],{"class":171},[106,723,214],{"class":126},[106,725,218],{"class":217},[106,727,221],{"class":207},[106,729,730],{"class":203}," finished.\"",[106,732,227],{"class":126},[106,734,736],{"class":108,"line":735},43,[106,737,142],{"emptyLinePlaceholder":141},[106,739,741],{"class":108,"line":740},44,[106,742,743],{"class":281},"# Upon step 'process_data' completion:\n",[106,745,747],{"class":108,"line":746},45,[106,748,749],{"class":281},"# 2. Serialization: `self.my_processed_data` (the list [2, 4, 6, 8, 10]) is serialized (e.g., pickled).\n",[106,751,753],{"class":108,"line":752},46,[106,754,755],{"class":281},"# 3. Hashing: A content hash (e.g., SHA1) is computed from the serialized bytes.\n",[106,757,759],{"class":108,"line":758},47,[106,760,761],{"class":281},"# 4. Persistence (Write to Datastore): The serialized bytes are written to the datastore,\n",[106,763,765],{"class":108,"line":764},48,[106,766,767],{"class":281},"#    often using the hash as part of its storage key. This is a datastore write operation.\n",[106,769,771],{"class":108,"line":770},49,[106,772,773],{"class":281},"# 5. Metadata Update: Metaflow's metadata service records info about this artifact:\n",[106,775,777],{"class":108,"line":776},50,[106,778,779],{"class":281},"#    flow, run ID, step name, task ID, and its content hash.\n",[15,781,782],{},"When we talk about the \"artifact lifecycle,\" we're describing the journey of a piece of data. This journey starts the moment you assign it to an attribute of self (like self.my_data = some_object) within a Metaflow step, continues as it's persisted in the datastore, and completes when it's retrieved for use in later steps or by external tools. Understanding this lifecycle, especially its interactions with the datastore, is key to grasping how Metaflow effectively manages data throughout your workflow.",[784,785,787],"h3",{"id":786},"stages-of-the-artifact-lifecycle","Stages of the Artifact Lifecycle",[15,789,790],{},"Let's break down the typical stages:",[792,793,794,804,821,827,833,839],"ol",{},[42,795,796,799,800,803],{},[27,797,798],{},"Creation & Assignment",": The lifecycle kicks off inside a step when a Python object is assigned to an instance variable of your flow class, for instance, ",[103,801,802],{},"self.my_model = trained_model_object",". At this exact moment, the object exists purely in the memory of the task executing that step.",[42,805,806,809,810,812,813,816,817,820],{},[27,807,808],{},"Serialization",": As soon as the step finishes successfully, Metaflow automatically takes all objects assigned to ",[103,811,187],{}," (like ",[103,814,815],{},"self.my_model",") and serializes them. This process, usually handled by Python's ",[103,818,819],{},"pickle"," module, converts your live Python objects into a byte stream. This byte stream format is what's suitable for storage and for sending across a network if needed.",[42,822,823,826],{},[27,824,825],{},"Hashing",": Right after serialization, a content hash (e.g., a SHA1 hash) is computed from this byte stream. As we discussed under \"Content-Addressed Storage,\" this hash acts as a unique fingerprint for the artifact's content.",[42,828,829,832],{},[27,830,831],{},"Persistence (Write to Datastore)",": The serialized byte stream is then written to Metaflow's configured datastore (like Amazon S3 or a local file system). This is a critical datastore write operation. The artifact is typically stored in a way that its content hash forms part of its storage path or key, directly linking its unique identity to its physical location.",[42,834,835,838],{},[27,836,837],{},"Metadata Update",": Simultaneously, Metaflow's metadata service (which could be running locally or be a remote service like the Metaflow Service) gets an update. This update logs essential information about the newly minted artifact: it links the artifact to the specific flow, run ID, step name, task ID, and, crucially, its content hash. This metadata acts like an index or a card catalog, allowing Metaflow to efficiently find and retrieve artifacts later on.",[42,840,841,844,845,847,848,851,852],{},[27,842,843],{},"Access (Read from Datastore)",": When a subsequent step in your flow needs this artifact (e.g., it tries to access ",[103,846,815],{},"), or when you access it using the Metaflow Client API (e.g., ",[103,849,850],{},"Run('MyFlow/123').data.my_model","), the process is essentially reversed:",[39,853,854,857,860],{},[42,855,856],{},"Metaflow first consults the metadata service to find the location of the required artifact, using the recorded hash.",[42,858,859],{},"It then retrieves the corresponding byte stream from the datastore. This is a datastore read operation.",[42,861,862,863,865,866,869],{},"Finally, this byte stream is deserialized (unpickled) back into a Python object in the memory of the task or client environment that requested it. This makes ",[103,864,815],{}," (or ",[103,867,868],{},"run.data.my_model",") available again, with its original content intact.",[15,871,872],{},"A vital aspect to remember is immutability. Once an artifact makes it to the datastore (Stage 4), it's considered unchangeable. If a step is re-run (perhaps due to a resume or a new run) and the value of self.my_model changes, this will trigger a new serialization, generate a new hash, and consequently, a new, distinct artifact will be created and stored. The original artifact from the previous attempt remains untouched in the datastore, associated with its original task.",[15,874,875],{},"The following diagram visually summarizes this lifecycle:",[15,877,878],{},[79,879],{"alt":880,"src":881},"Diagram of the Metaflow artifact lifecycle: Create & Assign -> Serialize -> Hash -> Persist to Datastore (write) -> Update Metadata -> Access from Datastore (read) -> Deserialize.","/content/learn/datastore-internals-02.png",[15,883,884],{},"This carefully orchestrated lifecycle ensures that your data is managed consistently, versioned effectively, and can be reliably passed between different parts of a Metaflow workflow, all underpinned by robust interactions with the datastore and the metadata service.",[68,886,887],{},[15,888,889],{},"When your Metaflow setup uses a remote datastore like Amazon S3, several of these interactions—such as uploading code packages, writing and reading artifacts, or any explicit S3 client calls within your steps—will involve network operations. The speed and efficiency of these operations can be influenced by factors like the size of your artifacts, your network bandwidth, and the current performance of S3.",[15,891,892],{},"How Metaflow Makes resume Possible",[15,894,895,896,899,900,902,903,905],{},"One of Metaflow's most valued features, especially when you're building complex or long-running pipelines, is the ability to ",[103,897,898],{},"resume"," a flow that failed or was interrupted. Instead of having to restart the entire computation from the very beginning—which can be incredibly time-consuming and costly—",[103,901,898],{}," allows the flow to pick up from the point of failure. This capability is a massive productivity booster. The ",[103,904,898],{}," mechanism is deeply connected to Metaflow's persistence strategy and relies on a few key components working together:",[784,907,909],{"id":908},"key-components-of-resumption","Key Components of Resumption",[792,911,912,921,938,951,960],{},[42,913,914,917,918,920],{},[27,915,916],{},"Step-as-Checkpoint",": As we've seen, every step in Metaflow inherently acts as a checkpoint. Upon the successful completion of any step, all its output artifacts (any data you've assigned to ",[103,919,187],{},") are durably persisted in the configured datastore. This ensures that the state of your flow is systematically saved at regular intervals.",[42,922,923,926,927,930,931,930,934,937],{},[27,924,925],{},"Tracking Run State",": The Metaflow metadata service meticulously keeps track of the status (e.g., ",[103,928,929],{},"running",", ",[103,932,933],{},"completed",[103,935,936],{},"failed",") of each individual step within every run. This provides a clear, auditable record of the flow's progress and allows Metaflow to pinpoint exactly where a failure occurred.",[42,939,940,943,944,946,947,950],{},[27,941,942],{},"Content-Addressing for Code and Data",": When you initiate a ",[103,945,898],{}," (e.g., by running ",[103,948,949],{},"python my_flow.py resume","), Metaflow first examines the state of the previously attempted run. For each step, it performs a crucial comparison: it looks at the content hash of the current code for that step and the hashes of its input artifact dependencies, and compares them against what was recorded for any previous successful execution of that same step within the same run lineage.",[42,952,953,956,957,959],{},[27,954,955],{},"Skipping the Already Done",": If the code for a step and its input artifacts (which are derived from successfully completed parent steps) haven't changed relative to a previously successful execution of that step in the current run lineage, Metaflow intelligently skips re-executing that step. It simply loads the previously computed output artifacts for that step directly from the datastore, presenting them to downstream steps as if the step had just run. This behavior is fundamental to ",[103,958,898],{}," and, as we'll see shortly, is also the core principle behind Metaflow's caching.",[42,961,962,965,966,968],{},[27,963,964],{},"Resuming from the Point of Failure",": If a step had previously failed, the ",[103,967,898],{}," command will attempt to re-execute that specific failed step (and, naturally, all subsequent steps in the Directed Acyclic Graph, or DAG). Importantly, the input artifacts that this failed step requires are loaded from the datastore, ensuring it starts with the exact same state as its previous, unsuccessful attempt.",[970,971,974],"callout",{"icon":972,"type":973},"i-heroicons-light-bulb-solid","info",[15,975,976],{},"The Core Benefit of resume: The most significant advantage of the resume feature is the substantial saving of computational time and resources. By cleverly avoiding the re-execution of parts of your workflow that have already completed successfully, you can iterate much faster, especially when debugging or dealing with transient issues.",[15,978,979],{},"Imagine this scenario:",[15,981,982],{},[79,983],{"alt":984,"src":985},"Diagram illustrating flow resumption: Step A and B completed, Step C failed. On resume, Step A and B are skipped (outputs loaded from datastore), and execution restarts from Step C.","/content/learn/datastore-internals-03.png",[15,987,988],{},"It's the combination of automatic artifact persistence (checkpointing), detailed metadata tracking, and content-addressing for both your data and your code that makes this intelligent resumption possible. This allows you to build more resilient pipelines and iterate on your work with greater speed and confidence.",[15,990,991],{},"Understanding Caching Behavior",[15,993,994],{},"Metaflow's caching mechanism is another powerful feature designed to significantly speed up your workflow execution. It achieves this by cleverly avoiding the recomputation of steps whose code and inputs haven't changed since a previous successful run. This behavior shares its foundational principles with the resume functionality, relying heavily on content-addressing and the persistent nature of artifacts in the datastore.",[784,996,998],{"id":997},"how-caching-works","How Caching Works",[15,1000,1001],{},"Before executing any step in your flow, Metaflow performs an internal check. It tries to determine if an identical version of that step—considering its code, its input artifacts, and implicitly its execution environment (like Python library versions defined via @conda or @pypi decorators)—has been successfully executed before within the same flow lineage (i.e., for the same flow name).",[15,1003,1004],{},"If all these conditions perfectly match a previously recorded successful execution, Metaflow achieves what we call a \"cache hit.\" In this scenario:",[792,1006,1007,1013],{},[42,1008,1009,1010,214],{},"The actual code execution for the step is ",[27,1011,1012],{},"skipped entirely",[42,1014,1015],{},"Metaflow directly retrieves the previously computed and stored output artifacts for that step from the datastore. These artifacts are then made available to any downstream steps, exactly as if the step had just run anew.",[15,1017,1018],{},"If any of these conditions do not match (a \"cache miss\"), the step is executed normally. Its new output artifacts are then persisted to the datastore, potentially becoming candidates for caching in future runs.",[784,1020,1022],{"id":1021},"conditions-for-a-cache-hit","Conditions for a Cache Hit",[15,1024,1025],{},"For Metaflow to decide to use a cached result for a step, the following conditions must generally hold true:",[39,1027,1028,1034,1040],{},[42,1029,1030,1033],{},[27,1031,1032],{},"Identical Code",": The content-hash of the current code for the step must be identical to the content-hash of the code from a previous successful execution of that step. Any change to your step's code will break the cache.",[42,1035,1036,1039],{},[27,1037,1038],{},"Identical Input Artifacts",": The content-hashes of all input artifacts for the current attempt of the step must perfectly match the content-hashes of the input artifacts from that same previous successful execution. If any upstream data changes, this step (and subsequent ones) will likely recompute.",[42,1041,1042,1045,1046,1049,1050,1053],{},[27,1043,1044],{},"Consistent Environment",": While not always hashed as a separate, explicit entity, the execution environment should be consistent. This includes factors like the Python version and, importantly, library versions managed by decorators like ",[103,1047,1048],{},"@conda"," or ",[103,1051,1052],{},"@pypi",". Changes in these can lead to different code behavior or artifact serialization, implicitly causing a cache miss even if your direct step code hasn't changed.",[784,1055,1057],{"id":1056},"benefits-of-caching","Benefits of Caching",[39,1059,1060,1066,1072],{},[42,1061,1062,1065],{},[27,1063,1064],{},"Saves Precious Computation Time",": By skipping redundant computations for steps that haven't changed, caching can dramatically reduce the overall runtime of your flows. This is especially true for steps involving intensive processing, data loading, or model training.",[42,1067,1068,1071],{},[27,1069,1070],{},"Conserves Computational Resources",": Avoiding re-execution means less demand on CPU, memory, and other resources. In cloud environments, this can translate directly into cost savings.",[42,1073,1074,1077],{},[27,1075,1076],{},"Accelerates Iterative Development",": Caching shines during the development and debugging phases of your project.",[970,1079,1080],{"icon":972,"type":973},[15,1081,1082],{},"Speed Up Your Iterations with Caching: Caching is a huge boon when you're actively developing or refining a specific part of your flow, perhaps a downstream modeling step. You can re-run the entire flow, and Metaflow will quickly skip through all the earlier, unchanged steps, allowing you to get to the part you're working on much faster. This enables rapid experimentation and significantly quicker feedback cycles.",[15,1084,1085],{},"The caching logic can be visualized as follows:",[15,1087,1088],{},[79,1089],{"alt":1090,"src":1091},"Flowchart of Metaflow caching logic: Check code hash -> Check input artifact hashes -> If both match previous success, use cached output; otherwise, execute step and store output.","/content/learn/datastore-internals-04.png",[15,1093,1094],{},"By smartly leveraging content-addressing and the robust persistence of artifacts, Metaflow's caching provides an intelligent way to optimize workflow execution, often without requiring any manual intervention or configuration from you, the developer.",[15,1096,1097],{},"Wrapping Up: Why Datastore Internals Matter to You",[15,1099,1100],{},"Metaflow's sophisticated dance with its datastore, built on principles like content-addressed storage and a meticulously defined artifact lifecycle, is far more than just an internal implementation detail. It's the very foundation of the framework's power, reliability, and developer-friendliness.",[784,1102,1104],{"id":1103},"key-features-enabled-by-datastore-internals","Key Features Enabled by Datastore Internals",[15,1106,1107],{},"As we've explored, the ability to uniquely identify, immutably store, and efficiently retrieve every artifact and even the code itself is what enables key features that you likely use every day:",[39,1109,1110,1116,1125],{},[42,1111,1112,1115],{},[27,1113,1114],{},"Efficient Caching",": Saving you time and computational resources by intelligently skipping already-computed steps.",[42,1117,1118,1124],{},[27,1119,1120,1121,1123],{},"Robust Flow Resumption (",[103,1122,898],{},")",": Allowing you to pick up right where you left off after failures, making your development process more resilient.",[42,1126,1127,1130],{},[27,1128,1129],{},"Reliable Versioning and Reproducibility",": Giving you confidence that you can always revisit past results and understand exactly how they were produced.",[784,1132,1134],{"id":1133},"empowering-your-workflow-development","Empowering Your Workflow Development",[15,1136,1137],{},"Understanding when and how Metaflow interacts with the datastore—from the initial packaging of your code to the persistence and retrieval of every single artifact—provides you with crucial insights. This knowledge helps in:",[39,1139,1140,1143,1146],{},[42,1141,1142],{},"Predicting workflow behavior.",[42,1144,1145],{},"Understanding performance characteristics (e.g., why a step might be slow if it's frequently reading/writing large artifacts to a remote store).",[42,1147,1148],{},"Developing more effective debugging strategies.",[15,1150,1151],{},"Ultimately, a solid grasp of these datastore internals empowers you to design more efficient, resilient, and debuggable Metaflow workflows. It allows you to fully harness the framework's capabilities to manage complex data pipelines with greater confidence and productivity.",[1153,1154,1155],"style",{},"html pre.shiki code .sNZSE, html code.shiki .sNZSE{--shiki-default:#39ADB5;--shiki-default-font-style:italic;--shiki-dark:#89DDFF;--shiki-dark-font-style:italic}html pre.shiki code .sv0dM, html code.shiki .sv0dM{--shiki-default:#90A4AE;--shiki-dark:#BABED8}html pre.shiki code .s4Nzc, html code.shiki .s4Nzc{--shiki-default:#39ADB5;--shiki-dark:#89DDFF}html pre.shiki code .sBv9x, html code.shiki .sBv9x{--shiki-default:#9C3EDA;--shiki-dark:#C792EA}html pre.shiki code .s2dLm, html code.shiki .s2dLm{--shiki-default:#E2931D;--shiki-dark:#FFCB6B}html pre.shiki code .sp2xM, html code.shiki .sp2xM{--shiki-default:#6182B8;--shiki-dark:#82AAFF}html pre.shiki code .swAhH, html code.shiki .swAhH{--shiki-default:#E53935;--shiki-default-font-style:italic;--shiki-dark:#F07178;--shiki-dark-font-style:italic}html pre.shiki code .s9-rc, html code.shiki .s9-rc{--shiki-default:#91B859;--shiki-dark:#C3E88D}html pre.shiki code .sRqct, html code.shiki .sRqct{--shiki-default:#F76D47;--shiki-dark:#F78C6C}html pre.shiki code .sbA4_, html code.shiki .sbA4_{--shiki-default:#E53935;--shiki-dark:#F07178}html pre.shiki code .s742E, html code.shiki .s742E{--shiki-default:#90A4AE;--shiki-default-font-style:italic;--shiki-dark:#676E95;--shiki-dark-font-style:italic}html .default .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .shiki span {color: var(--shiki-default);background: var(--shiki-default-bg);font-style: var(--shiki-default-font-style);font-weight: var(--shiki-default-font-weight);text-decoration: var(--shiki-default-text-decoration);}html .dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}html.dark .shiki span {color: var(--shiki-dark);background: var(--shiki-dark-bg);font-style: var(--shiki-dark-font-style);font-weight: var(--shiki-dark-font-weight);text-decoration: var(--shiki-dark-text-decoration);}",{"title":101,"searchDepth":138,"depth":138,"links":1157},[1158,1159],{"id":21,"depth":138,"text":22},{"id":92,"depth":138,"text":93,"children":1160},[1161,1162,1163,1164,1165,1166,1167],{"id":786,"depth":145,"text":787},{"id":908,"depth":145,"text":909},{"id":997,"depth":145,"text":998},{"id":1021,"depth":145,"text":1022},{"id":1056,"depth":145,"text":1057},{"id":1103,"depth":145,"text":1104},{"id":1133,"depth":145,"text":1134},"Explore how Metaflow interacts with the datastore, enabling features like caching and resume.","md",{},"Datastore, Caching & Resume","/learn/deep-dives/persistence/datastore-internals",{"title":5,"description":1168},"1.learn/1.deep-dives/1.persistence/2.datastore-internals","%s - Metaflow Persistence","EyN-AREer0nozYhWQNgpyY3dSc6gtaBunNnuFO5TmkU",[],[1179,1184],{"title":1180,"path":1181,"stem":1182,"navigationTitle":1183,"order":109,"children":-1},"Understanding Metaflow Persistence: Artifacts, Datastores, and Best Practices","/learn/deep-dives/persistence","1.learn/1.deep-dives/1.persistence/1.index","Persistence Overview",{"title":1185,"path":1186,"stem":1187,"navigationTitle":1188,"order":138,"children":-1},"Patterns for Metaflow","/learn/patterns","1.learn/2.patterns/1.index","Patterns",1749322578225]